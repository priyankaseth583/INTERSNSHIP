{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cc1d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.8.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24179e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "import urllib.request\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import re \n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28720b4",
   "metadata": {},
   "source": [
    "# Q1. Answer:- A python  program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6bb3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'chromedriver.exe')\n",
    "url=(\"https://www.amazon.in/\")\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_product=input(\"Enter the Product: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Product is: \",search_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfc6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "product=driver.find_element(By.ID,\"twotabsearchtextbox\").send_keys(search_product)\n",
    "search_product=driver.find_element(By.ID,\"nav-search-submit-button\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea2c47",
   "metadata": {},
   "source": [
    "#Q2. Answer:- A python program to scrape the details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   fetching URL to collect all the Product URLS \n",
    "product_url=[]\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,\"//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']\")\n",
    "    for i in url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "    #next_button=driver.find_elements(\"xpath\",'//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]').click()\n",
    "    next_4=driver.find_element(\"xpath\",'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[28]/div/div/span/span[1]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e019c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(product_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1766099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating emptylist \n",
    "Brand_name= []\n",
    "Product_name = []\n",
    "price=[]\n",
    "Return_exc = []\n",
    "Expected_Delivery = []\n",
    "Availability = []\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Data for Brand name    \n",
    "    try:\n",
    "        Brand=driver.find_element(By.XPATH,\"//span[@class='a-size-large product-title-word-break']\")\n",
    "        Brand_name.append(Brand.text.split()[0]+\" \"+Brand.text.split()[1])\n",
    "    except NoSuchElementException as e:\n",
    "        Brand_name.append('-')\n",
    "        \n",
    "    #  Data for Product name        \n",
    "    try:\n",
    "        Product=driver.find_element(By.XPATH,\"//span[@class='a-size-large product-title-word-break']\")\n",
    "        Product_name.append(Product.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Product_name.append('-')\n",
    "        \n",
    "    #  Data for Product's Price\n",
    "    try:\n",
    "        price_tag=driver.find_element(By.XPATH,\"//span[@class='a-price aok-align-center reinventPricePriceToPayMargin priceToPay']\")\n",
    "        price.append(price_tag.text.split(\"\\n\")[0])\n",
    "    except NoSuchElementException as e:\n",
    "        price.append('-')\n",
    "\n",
    "    #  Data for Return/Exchange \n",
    "    try:\n",
    "        Return_tag=driver.find_element(By.XPATH,\"//div[@id='RETURNS_POLICY']/span/div[2]/a\")\n",
    "        Return_exc.append(Return_tag.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Return_exc.append('-')\n",
    "\n",
    "    #  Data for Expected Delivery \n",
    "    try:\n",
    "        Delivery=driver.find_element(By.XPATH,\"//div[@id='mir-layout-DELIVERY_BLOCK-slot-PRIMARY_DELIVERY_MESSAGE_LARGE']\")\n",
    "        Expected_Delivery.append(Delivery.text.split('delivery')[1].replace(\"Details\",\" \"))\n",
    "    except NoSuchElementException as e:\n",
    "        Expected_Delivery.append('-')\n",
    "\n",
    "    #  Data for Expected Delivery \n",
    "    try:\n",
    "        Available=driver.find_element(By.XPATH,\"//div[@id='availability']\")\n",
    "        Availability.append(Available.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Availability.append(' - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brand_name),len(Product_name),len(price),len(Return_exc),len(Expected_Delivery),len(Availability),len(product_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "products=pd.DataFrame({\"Brand_name\":Brand_name,\"Product Name\":Product_name,\"Product Price\":price,\"Return/Exchange\":Return_exc,\n",
    "                       \"Expected Delivery\":Expected_Delivery,\"Availability\":Availability, \"Product URL\":product_url})\n",
    "print(products)\n",
    "\n",
    "#saving to csv file\n",
    "laptop=products.to_csv('laptop.csv')\n",
    "laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d100d0d",
   "metadata": {},
   "source": [
    "#Q3. Answer:- A python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# maximizing the browser size\n",
    "driver.maximize_window()\n",
    "\n",
    "# Website \"images.google.com\"\n",
    "url = \"https://images.google.com/\"\n",
    "time.sleep(2)\n",
    "    \n",
    "# Empty list and giving search items as list and creating loop\n",
    "urls = []    \n",
    "data = []\n",
    "search_item = [\"fruits\", \"cars\", \"Machine Learning\",\"guitar\",\"cakes\"]\n",
    "for item in search_item:\n",
    "    driver3.get(url)  \n",
    "    time.sleep(5)\n",
    "    search_img=driver.find_element(By.XPATH,'//input[@class=\"gLFyf gsfi\"]').send_keys(item)  #sending key word for search item\n",
    "    \n",
    "    search_btn = driver.find_element(By.XPATH,'//button[@class=\"Tg7LZd\"]').submit() #Clicking on search button\n",
    "    \n",
    "    # scrolling the web page to get more images\n",
    "    for _ in range(20):\n",
    "        driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "        \n",
    "        imgs = driver.find_elements(By.XPATH,\"//img[@class='rg_i Q4LuWd']\")\n",
    "    img_url = []\n",
    "    for image in imgs:\n",
    "        source = image.get_attribute('src')\n",
    "        if source is not None:\n",
    "                if(source[0:4] == 'http'):\n",
    "                    img_url.append(source)\n",
    "    for i in img_url[:10]:\n",
    "        urls.append(i)\n",
    "                    \n",
    "for i in range(len(urls)):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 10))\n",
    "    response = requests.get(urls[i])\n",
    "file = open(r\"C:\\Users\\sunee\\Desktop\\FliproboInternship\\Webscraping\"+str(i)+\".jpg\", \"wb\")\n",
    "file.write(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b8b44",
   "metadata": {},
   "source": [
    "#Q.4.Answer: A python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1c9d20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25200\\4286317870.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'chromedriver.exe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.flipkart.com/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpop_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xpath\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"/html/body/div[2]/div/div/button\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r'chromedriver.exe')\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(2)\n",
    "driver.maximize_window()\n",
    "pop_close = driver.find_element(\"xpath\",\"/html/body/div[2]/div/div/button\").click()\n",
    "product = input(\" Enter the name of Smartphone that you want to Search : \")\n",
    "search_product=driver.find_element(By.CLASS_NAME,'_3704LK').send_keys(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button= driver.find_element(By.CLASS_NAME,'L0Z3Pu').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls of phones coming on 1st page\n",
    "page1_urls = []\n",
    "urls = driver.find_elements(By.XPATH,'//a[@class=\"_1fQZEK\"]')\n",
    "for url in urls:\n",
    "    page1_urls.append(url.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8afdf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  creating  empty  list\n",
    "Brand = []\n",
    "Phone_name = []\n",
    "Colour = []\n",
    "RAM= []\n",
    "ROM = []\n",
    "Primary_Camera= []\n",
    "Secondary_Camera = []\n",
    "Display_Size= []\n",
    "Battery_Capacity = []\n",
    "Price = []\n",
    "URL = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8256f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from each url of page 1\n",
    "for url in page1_urls:\n",
    "    driver4.get(url)              \n",
    "    URL.append(url)                                                          \n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #Clicking on read more button\n",
    "    try:\n",
    "        read_more = driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _1FH0tX\"]')     \n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception occured while moving to next page\")\n",
    "        \n",
    "    \n",
    "    #Scraping brand name of phone data\n",
    "    try:\n",
    "        brand_tags = driver.find_element(By.XPATH,'//span[@class=\"B_NuCI\"]')      \n",
    "        Brand.append(brand_tags.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping phone name data\n",
    "    try:\n",
    "        name_tags = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')     \n",
    "        Phone_name.append(name_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Phone_name.append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping phone color data\n",
    "    try:\n",
    "        color_tags = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      \n",
    "        Colour.append(color_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Colour.append('-')\n",
    "     \n",
    "    \n",
    "    #Scraping RAM data\n",
    "    try:\n",
    "        ram_tags = driver4.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                \n",
    "        RAM.append(ram_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        RAM.append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping ROM data\n",
    "    try:\n",
    "        rom_tags = driver4.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li')        \n",
    "        ROM.append(rom_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        ROM.append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping Primary camera data\n",
    "    try:                                                                                    \n",
    "        pri_tags = driver4.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        Primary_Camera.append(pri_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Primary_Camera.append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping secondary camera data\n",
    "    try:                                                                                    \n",
    "        sec_tags = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[1]')\n",
    "        if sec_tags != \"Secondary Camera\" : \n",
    "            if driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[1]').text == \"Secondary Camera\":\n",
    "                sec_cam = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li')\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[2]/ul/li')\n",
    "        Secondary_Camera.append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        Secondary_Camera.append('-')\n",
    "        \n",
    "     #Scraping Display size data \n",
    "    try:\n",
    "        disp_tags = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_tags.text != \"Displa Size\" : raise NoSuchElementException\n",
    "        disp_size = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[1]/td[2]/ul/li')  \n",
    "        Display_Size.append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        Display_Size.append('-')\n",
    "        \n",
    "        #Scraping battery capacity data\n",
    "    try:\n",
    "        if driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[1]')\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[2]/ul/li')                \n",
    "            elif driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[1]')\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_tags = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[1]')\n",
    "            if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_capa = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[2]/ul/li')              \n",
    "        Battery_Capacity.append(bat_capa.text)\n",
    "    except NoSuchElementException:\n",
    "        Battery_Capacity.append('-')        \n",
    "        \n",
    "    #Scraping Price data\n",
    "    try:\n",
    "        price_tags = driver.find_element(By.XPATH,'//div[@class=\"_30jeq3 _16Jk6d\"]')      \n",
    "        Price.append(price_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7308900",
   "metadata": {},
   "outputs": [],
   "source": [
    "smartphone=pd.DataFrame({\"brand_name\":Brand[:24],\"smartphone_name\":Phone_name[:24],\"product_colour\":Colour[:24],\"product_RAM\":RAM[:24],\n",
    "                         \"product_ROM\":ROM[:24],\"Primary_Camera\":Primary_Camera[:24],\"Secondary_Camera\":Secondary_Camera[:24],\n",
    "                         \"Display_size\":Display_Size[:24],\"Battery_Capacity\":Battery_Capacity[:24],\"Phone_Price\":Price[:24],\"Product_URL\":URL[:24]})\n",
    "\n",
    "smartphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae360f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_phone=smartphone.to_csv('smart_phone.csv')\n",
    "smart_phone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd45e33",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening google maps web page\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "\n",
    "#Sending keyword for seach box and search button\n",
    "city = input('Enter City name to search : ')\n",
    "search_city = driver.find_element(By.ID,\"searchboxinput\")                       \n",
    "search_city.clear()                                                             \n",
    "time.sleep(2)\n",
    "\n",
    "search_city.send_keys(city)                                                     \n",
    "search_button = driver.find_element(By.ID,\"searchbox-searchbutton\").click()                                                             \n",
    "time.sleep(3)\n",
    "Location=driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b25908",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    coordinates=re.findall(r'@(.*)data',Location)\n",
    "    lat_lon=coordinates[0].split(\",\")\n",
    "    lat=lat_lon[0]\n",
    "    lon=lat_lon[1]\n",
    "except IndexError as e:\n",
    "    coordinates=re.findall(r'@(.*)z',Location)\n",
    "    lat_lon=coordinates[0].split(\",\")\n",
    "    lat=lat_lon[0]\n",
    "    lon=lat_lon[1]\n",
    "print(\"\\nlatitude={}\".format(lat))\n",
    "print(\"\\nlongitude={}\".format(lon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994efc5d",
   "metadata": {},
   "source": [
    "# Q.7.Answer: Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening Digit.in web page\n",
    "driver.get(\"https://digit.in/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c03e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,\"/html/body/div[3]/div/div[2]/div[2]/div[4]/ul/li[9]/a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name=[]\n",
    "seller=[]\n",
    "price=[]\n",
    "\n",
    "product_tag=driver.find_elements(By.XPATH,\"//*[@id='summtable']/tbody/tr/td[1]\")\n",
    "for i in product_tag:\n",
    "    try:\n",
    "        product_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        product_name.append(\"-\")\n",
    "\n",
    "seller_tag=driver.find_elements(By.CLASS_NAME,\"smmerchant\")\n",
    "for i in seller_tag:\n",
    "    try:\n",
    "        seller.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        seller.append(\"-\")\n",
    "\n",
    "price_tag=driver.find_elements(By.CLASS_NAME,\"smprice\")\n",
    "for i in price_tag:\n",
    "    try:\n",
    "        price.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        price.append(\"-\")\n",
    "\n",
    "Gaming=pd.DataFrame({\"Product Name\":product_name,\"Seller\":seller,\"Price\":price})\n",
    "Gaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc505b",
   "metadata": {},
   "source": [
    "# Q.8. Answer: Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening web page\n",
    "driver.get(\"https://www.forbes.com/billionaires/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty list\n",
    "rank = []\n",
    "name = []\n",
    "net_worth = []\n",
    "age = []\n",
    "citzn = []\n",
    "source = []\n",
    "industry = []\n",
    "\n",
    "#Fetching Rank\n",
    "rank_tag = driver.find_elements(By.XPATH,\"//div[@class='rank']\")\n",
    "for r in rank_tag:\n",
    "    try:\n",
    "        rank.append(r.text)\n",
    "    except NoSuchElementException:\n",
    "        rank.append(\"-\")\n",
    "    \n",
    "#Fetching Name\n",
    "name_tag = driver.find_elements(By.XPATH,\"//div[@class='personName']\")\n",
    "for n in name_tag:\n",
    "    try:\n",
    "        name.append(n.text)\n",
    "    except NoSuchElementException:\n",
    "        name.append(\"-\")\n",
    "        \n",
    "#Fetching Net Worth\n",
    "netwrth_tag = driver.find_elements(By.XPATH,\"//div[@class='netWorth']\")\n",
    "for nt in netwrth_tag:\n",
    "    try:\n",
    "        net_worth.append(nt.text)\n",
    "    except NoSuchElementException:\n",
    "        net_worth.append(\"-\")\n",
    "        \n",
    "#Fetching Age\n",
    "age_tag = driver.find_elements(By.XPATH,\"//div[@class='age']\")\n",
    "for a in age_tag:\n",
    "    try:\n",
    "        age.append(a.text)\n",
    "    except NoSuchElementException:\n",
    "        age.append(\"-\")\n",
    "    \n",
    "#Fetching Citizenship\n",
    "cit_tag = driver.find_elements(By.XPATH,\"//div[@class='countryOfCitizenship']\")\n",
    "for c in cit_tag:\n",
    "    try:\n",
    "        citzn.append(c.text)\n",
    "    except NoSuchElementException:\n",
    "        citzn.append(\"-\")\n",
    "    \n",
    "#Fetching Source\n",
    "src_tag = driver.find_elements(By.XPATH,\"//span[@class='source-text']\")\n",
    "for s in src_tag:\n",
    "    try:\n",
    "        source.append(s.text)\n",
    "    except NoSuchElementException:\n",
    "        source.append(\"-\")\n",
    "    \n",
    "#Fetching Industry\n",
    "ind_tag = driver.find_elements(By.XPATH,\"//div[@class='category']\")\n",
    "for ind in ind_tag:\n",
    "        try:\n",
    "            industry.append(ind.text)\n",
    "        except NoSuchElementException:\n",
    "            industry.append(\"-\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of all the coloumns\n",
    "len(rank),len(name),len(net_worth),len(age),len(citzn),len(source),len(industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50789362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Dataframe\n",
    "Billionaires =pd.DataFrame({'Rank':rank,'Name':name,'Net Worth':net_worth,'Age': age,'Citizenship/Country':citzn,'Source':source,'Industry':industry})\n",
    "Billionaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d032205",
   "metadata": {},
   "source": [
    "# Q.9. Answer: Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72882d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get(\"https://www.youtube.com/watch?v=USccSZnS8MQ\")\n",
    "driver.maximize_window()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbded5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while(i<100):\n",
    "    driver.execute_script(\"window.scrollBy(0,500)\") # scroll down to get more comments\n",
    "    i+=1\n",
    "while(i<402):\n",
    "    driver.execute_script(\"window.scrollBy(0,5000)\") # scroll down to get more comments\n",
    "    i+=1\n",
    "    \n",
    "comment = []\n",
    "upvote = []\n",
    "comment_time = []\n",
    "\n",
    "comment_tag=(driver.find_elements(By.XPATH,'//yt-formatted-string[@id=\"content-text\"]'))\n",
    "for i in comment_tag:             \n",
    "    try:\n",
    "        comment.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        comment.append(\"-\")\n",
    "\n",
    "upvote_tag=(driver.find_elements(By.XPATH,\"//*[@id='vote-count-middle']\"))\n",
    "for i in upvote_tag:             \n",
    "    try:\n",
    "        upvote.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        upvote.append(\"-\")\n",
    "\n",
    "comment_time_tag=(driver.find_elements(By.XPATH,\"//*[@id='header-author']/yt-formatted-string/a\"))\n",
    "for i in comment_time_tag:             \n",
    "    try:\n",
    "        comment_time.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        comment_time.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "video=pd.DataFrame({\"Comment\":comment,\"Upvote\":upvote,\"Comment_ Time\":comment_time})\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f498c",
   "metadata": {},
   "source": [
    "# Q.10. Answer: Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get(\"https://www.hostelworld.com/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.ID,'location-text-input-field').click()\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH,'//input[@id=\"location-text-input-field\"]').send_keys('London')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"predicted-search-results\"]/li[1]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(\"xpath\",'//div[@class=\"search-button\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cd015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make empty lists\n",
    "Hostel_Name = []\n",
    "Distance = []\n",
    "overall_review = []\n",
    "total_reviews = []\n",
    "facilities = []\n",
    "price = []\n",
    "Rating = []\n",
    "property_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e62f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    \n",
    "    # Hostel name\n",
    "    names = driver.find_elements(By.XPATH,'//h2[@class=\"title title-6\"]')\n",
    "    for name in names:\n",
    "        Hostel_Name.append(name.text)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    # Distance from city\n",
    "    distance = driver.find_elements(By.XPATH,'//span[@class=\"description\"]')\n",
    "    for d in distance:\n",
    "        Distance.append(d.text)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    #Review    \n",
    "    review = driver.find_elements(By.XPATH,'//div[@class=\"keyword\"]//span')\n",
    "    for r in review:\n",
    "        overall_review.append(r.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Total number of reviews     \n",
    "    t_review = driver.find_elements(By.XPATH,'//div[@class=\"reviews\"]')\n",
    "    for t in t_review:\n",
    "        total_reviews.append(t.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # facilities\n",
    "    service = driver.find_elements(By.XPATH,'//div[@class=\"facilities-label facilities\"]')\n",
    "    for s in service:\n",
    "        facilities.append(s.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Prices    \n",
    "    prices = driver.find_elements(By.XPATH,'//div[@class=\"price-col\"]')\n",
    "    for p in prices:\n",
    "        price.append(p.text)\n",
    "    time.sleep(2)    \n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,'//div[@class=\"pagination-item pagination-next\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "time.sleep(2) \n",
    "\n",
    "# Separate  Privates_From price  and  Dorms_From price\n",
    "private = []\n",
    "for i in range(0,len(price),2):\n",
    "    private.append(price[i])\n",
    "time.sleep(2)\n",
    "\n",
    "dorms = []\n",
    "for i in range(1,len(price),2):\n",
    "    dorms.append(price[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape Hostels URL\n",
    "hostel_url = []\n",
    "\n",
    "while(True):\n",
    "    urls = driver.find_elements(By.XPATH,'//h2[@class=\"title title-6\"]/a')\n",
    "    for url in urls:\n",
    "        hostel_url.append(url.get_attribute(\"href\"))\n",
    "    time.sleep(2)    \n",
    "        \n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,'//div[@class=\"pagination-item pagination-prev\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "Rate = []\n",
    "for page in hostel_url:\n",
    "    driver0.get(page)\n",
    "    \n",
    "    # Rating\n",
    "    try:\n",
    "        ratings = driver.find_element(By.XPATH,'//*[@id=\"__layout\"]/div/div[1]/section/div[6]/div/div[1]/div[1]/div[1]')\n",
    "        Rate.append(ratings.text)\n",
    "    except NoSuchElementException:\n",
    "        Rate.append(\"No Rating\")  \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Property Description\n",
    "    try:\n",
    "        pd = driver.find_element(By.XPATH,'//*[@id=\"__layout\"]/div/div[1]/section/div[6]/div/div[2]/div[2]/div/div[2]')\n",
    "        property_description.append(pd.text)\n",
    "    except NoSuchElementException:\n",
    "        property_description.append(\"No Description\")  \n",
    "\n",
    "    \n",
    "time.sleep(2)        \n",
    "# remove extra data from Rating     \n",
    "all_text = []\n",
    "for i in Rate:\n",
    "    all_text.append(i.split())\n",
    "time.sleep(2)\n",
    "\n",
    "for i in all_text:\n",
    "    Rating.append(i[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(Hostel_Name),len(Distance),len(Rating),len(total_reviews),len(overall_review),\n",
    "    len(private),len(dorms),len(facilities),len(property_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hostel=pd.DataFrame({\"Hostel Name\":Hostel_Name,\"Distance\":Distance,\"Rating\":Rating,\"Total Reviews\":total_reviews,\"Overall Review\":overall_review[:30],\n",
    "                     \"Private Price\":private,\"Dorms Price\":dorms,\"Facilities\":facilities,\"Property Description\":property_description})\n",
    "Hostel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hostels=Hostel.to_csv('Hostels.csv')\n",
    "Hostels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
